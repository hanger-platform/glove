<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>glove</name>
  <description />
  <extended_description />
  <job_version />
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2017/04/10 20:20:28.203</created_date>
  <modified_user>-</modified_user>
  <modified_date>2017/04/10 20:20:28.203</modified_date>
  <parameters>
    <parameter>
      <name>ALLOW_DUPLICATED</name>
      <default_value>0</default_value>
      <description>Identifica se permite chaves duplicadas nos arquivos de dados colunares. </description>
    </parameter>
    <parameter>
      <name>ALLOW_RECREATE</name>
      <default_value>1</default_value>
      <description>Identifica se uma tabela pode ser recriada quando houver mudança de estrutura.</description>
    </parameter>
    <parameter>
      <name>BIG_QUERY_PROJECT_ID</name>
      <default_value />
      <description>Identifica o projeto  do Big Query que será utilizado.</description>
    </parameter>
    <parameter>
      <name>CONNECTION_NAME</name>
      <default_value />
      <description>Identifica o nome de uma conexão contida no arquivo ~/.kettle/connections.properties.</description>
    </parameter>
    <parameter>
      <name>CUSTOM_PRIMARY_KEY</name>
      <default_value />
      <description>Identifica a chave única de cada registro.</description>
    </parameter>
    <parameter>
      <name>DEBUG</name>
      <default_value>0</default_value>
      <description>Identifica se a execução será realizada em modo de debug, mantendo todos os arquivos temporários gerados.</description>
    </parameter>
    <parameter>
      <name>DELTA_DELAY</name>
      <default_value>60</default_value>
      <description>Identifica o tempo em minutos que deve ser considerado na gravação do delta para evitar que itens sendo comitados não sejam extraídos.</description>
    </parameter>
    <parameter>
      <name>DELTA_FIELD</name>
      <default_value />
      <description>Identifica o campo de delta.</description>
    </parameter>
    <parameter>
      <name>DELTA_VALUE</name>
      <default_value />
      <description>Identifica o valor do delta.</description>
    </parameter>
    <parameter>
      <name>DISTKEY</name>
      <default_value />
      <description>Identifica a chave de distribuição da tabela. </description>
    </parameter>
    <parameter>
      <name>DISTSTYLE</name>
      <default_value>none</default_value>
      <description>Identifica o estilo de distribuição do redshift.</description>
    </parameter>
    <parameter>
      <name>ENCODE</name>
      <default_value>zstd</default_value>
      <description>Identifica o tipo de compressão aplicado nas colunas de tabelas do redshift</description>
    </parameter>
    <parameter>
      <name>EXPORT_BUCKET_DEFAULT</name>
      <default_value />
      <description>Identifica o bucket default para exportação de dados. </description>
    </parameter>
    <parameter>
      <name>EXPORT_SHEETS_METHOD</name>
      <default_value>0</default_value>
      <description>Identifica o tipo de atualização da planilha [0: FULL, 1:APPEND]</description>
    </parameter>
    <parameter>
      <name>EXPORT_SHEET_DEFAULT</name>
      <default_value />
      <description>Identifica o nome da página (guia)</description>
    </parameter>
    <parameter>
      <name>EXPORT_SPREADSHEET_DEFAULT</name>
      <default_value />
      <description>Identifica o Id da spreadsheet</description>
    </parameter>
    <parameter>
      <name>FIELD_HAS_PREFIX</name>
      <default_value>0</default_value>
      <description>Identifica se o prefixo dos campos das tabelas do HANA devem ser mantidos. </description>
    </parameter>
    <parameter>
      <name>FILE_DONE_BUCKET</name>
      <default_value />
      <description>Identifica o diretório para armazenamento de arquivos processados.</description>
    </parameter>
    <parameter>
      <name>FILE_INPUT_BUCKET</name>
      <default_value />
      <description>Identifica o diretório de entrada de arquivos a serem processados.</description>
    </parameter>
    <parameter>
      <name>FILE_INPUT_DELIMITER</name>
      <default_value>;</default_value>
      <description>Identifica o delimitador do arquivo de entrada. </description>
    </parameter>
    <parameter>
      <name>FILE_INPUT_EXTENSION</name>
      <default_value>csv</default_value>
      <description>Identifica o tipo de arquivos de entrada.</description>
    </parameter>
    <parameter>
      <name>FILE_INPUT_MANIFEST</name>
      <default_value />
      <description>Identifica o arquivo de manifesto que deve ser usado para definição do metadado.</description>
    </parameter>
    <parameter>
      <name>FILE_INPUT_PARTITONED</name>
      <default_value>0</default_value>
      <description>Identifica o tipo de particionamento do arquivo.</description>
    </parameter>
    <parameter>
      <name>FILE_OUTPUT_MODE</name>
      <default_value>append</default_value>
      <description>Identifica o mode de atualização da tabela. </description>
    </parameter>
    <parameter>
      <name>FILE_OUTPUT_SCHEMA</name>
      <default_value />
      <description>Identifica o schema de destino.</description>
    </parameter>
    <parameter>
      <name>FILE_OUTPUT_TABLE</name>
      <default_value />
      <description>Identifica a tabela de destino.</description>
    </parameter>
    <parameter>
      <name>GENERIC_PARAMETER</name>
      <default_value />
      <description>Identifica a lista de parâmetros que serão passados para uma named query.</description>
    </parameter>
    <parameter>
      <name>GOOGLE_CLOUD_BUCKET</name>
      <default_value />
      <description>Identifica o bucket a ser utilizado no Google Cloud.</description>
    </parameter>
    <parameter>
      <name>HAS_ATHENA</name>
      <default_value>0</default_value>
      <description>Identifica se o AWS Athena deve ser usado sempre que disponível.</description>
    </parameter>
    <parameter>
      <name>INPUT_TABLE_NAME</name>
      <default_value />
      <description>Identifica o nome da tabela de origem dos dados.</description>
    </parameter>
    <parameter>
      <name>INPUT_TABLE_SCHEMA</name>
      <default_value />
      <description>Identifica o nome do schema de origem dos dados.</description>
    </parameter>
    <parameter>
      <name>IS_EXPORT</name>
      <default_value>0</default_value>
      <description>Identifica se será gerado o export do resultset.</description>
    </parameter>
    <parameter>
      <name>IS_PARALLEL</name>
      <default_value>0</default_value>
      <description>Identifica se será utilizado processamento paralelo para particionamento de arquivos. </description>
    </parameter>
    <parameter>
      <name>IS_RECREATE</name>
      <default_value>0</default_value>
      <description>Identifica se deve recriar a tabela. </description>
    </parameter>
    <parameter>
      <name>IS_RELOAD</name>
      <default_value>0</default_value>
      <description>Identifica se deve recarregar o conteúdo da tabela.</description>
    </parameter>
    <parameter>
      <name>IS_SPECTRUM</name>
      <default_value>1</default_value>
      <description>Identifica se o AWS Redshift Spectrum está configurado.</description>
    </parameter>
    <parameter>
      <name>MANIFEST</name>
      <default_value />
      <description>Identifica se o manifest será passado por parâmetro.</description>
    </parameter>
    <parameter>
      <name>METADATA_BLACKLIST</name>
      <default_value />
      <description>Identifica a lista de campos a serem excluídos (apenas para database module)</description>
    </parameter>
    <parameter>
      <name>MODULE</name>
      <default_value>database</default_value>
      <description>Identifica o módulo que será executado</description>
    </parameter>
    <parameter>
      <name>NAMED_QUERY</name>
      <default_value />
      <description>Identifica a query origem dos dados. </description>
    </parameter>
    <parameter>
      <name>NAMED_QUERY_DIRECTORY</name>
      <default_value> </default_value>
      <description>Identifica o diretório de origem das named queries.</description>
    </parameter>
    <parameter>
      <name>NAMED_QUERY_IGNORE_STEP</name>
      <default_value />
      <description>Identifica os passos que devem ser ignorados na execução de uma named query. </description>
    </parameter>
    <parameter>
      <name>NAMED_QUERY_INCLUDE_STEP</name>
      <default_value />
      <description>Identifica os passos que devem ser considerados na execução de uma named query.</description>
    </parameter>
    <parameter>
      <name>ONLY_EXPORT</name>
      <default_value>0</default_value>
      <description>Identifica se o processo apenas exporta o resultado sem materializar uma tabela. </description>
    </parameter>
    <parameter>
      <name>OUTPUT_COMPRESSION</name>
      <default_value>gzip</default_value>
      <description>Identifica o tipo de compressão dos arquivos de saída. </description>
    </parameter>
    <parameter>
      <name>OUTPUT_FORMAT</name>
      <default_value>parquet</default_value>
      <description>Identifica o formato de arquivo de saída. </description>
    </parameter>
    <parameter>
      <name>OUTPUT_TABLE_NAME</name>
      <default_value />
      <description>Identifica o nome da tabela que será criada.</description>
    </parameter>
    <parameter>
      <name>OUTPUT_TABLE_SCHEMA</name>
      <default_value />
      <description>Identifica o nome do schema que será criado.</description>
    </parameter>
    <parameter>
      <name>PARTITION_EAGER</name>
      <default_value>1</default_value>
      <description>Identifica se deve extrair todo o dado, quando 1, ou uma partição por vez, quando 0.</description>
    </parameter>
    <parameter>
      <name>PARTITION_FIELD</name>
      <default_value />
      <description>Identifica o campo de negócio que será utilizado para o particinamento dos dados.</description>
    </parameter>
    <parameter>
      <name>PARTITION_FORMAT</name>
      <default_value>yyyymm</default_value>
      <description>Identifica o formato da partição quando o particionamento for feito por data [yyyy, yyyymm, yyyymmdd].</description>
    </parameter>
    <parameter>
      <name>PARTITION_HAS_PREFIX</name>
      <default_value>1</default_value>
      <description>Identifica se o campo de partição da tabela do SAP tem zero como prefixo. </description>
    </parameter>
    <parameter>
      <name>PARTITION_LENGTH</name>
      <default_value>1000000</default_value>
      <description>Identifica a quantidade de registros de cada partição.</description>
    </parameter>
    <parameter>
      <name>PARTITION_MERGE</name>
      <default_value>1</default_value>
      <description>Identifica se deve ser realizado merge dos dados da partição. </description>
    </parameter>
    <parameter>
      <name>PARTITION_MINIMUN</name>
      <default_value>2015</default_value>
      <description>Identifica o valor mínimo para definição de range de partição de dada.</description>
    </parameter>
    <parameter>
      <name>PARTITION_MODE</name>
      <default_value>virtual</default_value>
      <description>Identifica o tipo de partiticionamento que será utilizado. </description>
    </parameter>
    <parameter>
      <name>PARTITION_TYPE</name>
      <default_value>timestamp</default_value>
      <description>Identifica o tipo da partição [id, date, timestamp].</description>
    </parameter>
    <parameter>
      <name>QUEUE_FILES_SIZE_LIMIT</name>
      <default_value>100000000</default_value>
      <description>Identifica o tamanho máximo em MB dos arquivos temporários. </description>
    </parameter>
    <parameter>
      <name>QUOTE</name>
      <default_value>1</default_value>
      <description>Identifica se o unload de dados do Redshift devem conter aspas duplas</description>
    </parameter>
    <parameter>
      <name>QUOTE_ESCAPE</name>
      <default_value>"</default_value>
      <description>Identifica o caracter utilizado para escape de aspas duplas.</description>
    </parameter>
    <parameter>
      <name>SAMPLE</name>
      <default_value>100000</default_value>
      <description>Identifica o sample de dados que é analisado para geração de metadado. </description>
    </parameter>
    <parameter>
      <name>SISENSE_CUBE</name>
      <default_value />
      <description>Identifica o cubo do sisense no qual as queries serão realizadas. </description>
    </parameter>
    <parameter>
      <name>SORTKEY</name>
      <default_value />
      <description>Identifica a chave de distribuição do Redshift.</description>
    </parameter>
    <parameter>
      <name>SPLIT_STRATEGY</name>
      <default_value>SECURE</default_value>
      <description>Identifica a estratégia de particionamento entre SECURE e FAST.</description>
    </parameter>
    <parameter>
      <name>STAGING_SCHEMA</name>
      <default_value>transient</default_value>
      <description>Identifica o schema utilizado como staging.</description>
    </parameter>
    <parameter>
      <name>STORAGE_BUCKET</name>
      <default_value />
      <description>Identifica o bucket do storage que será utilizado para transferência de arquivos.</description>
    </parameter>
    <parameter>
      <name>STORAGE_BUCKET_BACKUP</name>
      <default_value />
      <description>Identifica o bucket do storage que será utilizado para backup de arquivos.</description>
    </parameter>
    <parameter>
      <name>TARGET</name>
      <default_value>spectrum</default_value>
      <description>Identifica o destino dos dados [spectrum, redshift ou bigquery].</description>
    </parameter>
    <parameter>
      <name>THREAD</name>
      <default_value>4</default_value>
      <description>Identifica a quantidade de threads que serão utilizadas pelo conversor de dados.</description>
    </parameter>
    <parameter>
      <name>TIMEZONE_OFFSET</name>
      <default_value />
      <description>Identifica o timezone offset para os campos do tipo timestamp.</description>
    </parameter>
    <parameter>
      <name>WHERE_CONDITION_TO_DELTA</name>
      <default_value />
      <description>Identifica a condição para carga delta. </description>
    </parameter>
    <parameter>
      <name>WHERE_CONDITION_TO_RECOVER</name>
      <default_value>1=1</default_value>
      <description>Identifica a condição para recuperação de desastres. </description>
    </parameter>
  </parameters>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection />
    <schema />
    <table />
    <size_limit_lines />
    <interval />
    <timeout_days />
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>N</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>N</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>N</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>N</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection />
    <schema />
    <table />
    <timeout_days />
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>N</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>N</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection />
    <schema />
    <table />
    <timeout_days />
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file />
  <entries>
    <entry>
      <name>START</name>
      <description />
      <type>SPECIAL</type>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>Module selector</name>
      <description />
      <type>JOB</type>
      <specification_method>filename</specification_method>
      <job_object_id />
      <filename>${Internal.Job.Filename.Directory}/j_${MODULE}_module.kjb</filename>
      <jobname />
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Nothing</loglevel>
      <slave_server_name />
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <expand_remote_job>N</expand_remote_job>
      <create_parent_folder>N</create_parent_folder>
      <pass_export>N</pass_export>
      <run_configuration />
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <set_append_logfile>N</set_append_logfile>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>480</yloc>
    </entry>
    <entry>
      <name>Metadata folder exists?</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory />
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>if [ -d "${GLOVE_METADATA}" ]; then
	echo "Metadata directory at ${GLOVE_METADATA} : )"	
else
	echo "Metadata directory does not exist, create folder ${GLOVE_METADATA} or check if your File System service is on :/"	
	exit 1
fi</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>256</yloc>
    </entry>
    <entry>
      <name>Setup</name>
      <description />
      <type>WRITE_TO_LOG</type>
      <logmessage>   _____   _                        
  / ____| | |                       
 | |  __  | |   ___   __   __   ___ 
 | | |_ | | |  / _ \  \ \ / /  / _ \
 | |__| | | | | (_) |  \ V /  |  __/
  \_____| |_|  \___/    \_/    \___| 
                                    
#-----------------------------------------------------------
# PATH
#-----------------------------------------------------------
GLOVE_HOME				== ${GLOVE_HOME}
GLOVE_TEMP				== ${GLOVE_TEMP}
GLOVE_METADATA				== ${GLOVE_METADATA}
GLOVE_NAMED_QUERIES			== ${GLOVE_NAMED_QUERIES}
GOOGLE_TOOLS_HOME			== ${GOOGLE_TOOLS_HOME}
GLOVE_STORARE_BUCKET_STAGING		== ${GLOVE_STORARE_BUCKET_STAGING}
GLOVE_STORARE_BUCKET_DISASTER_RECOVERY	== ${GLOVE_STORARE_BUCKET_DISASTER_RECOVERY}
#-----------------------------------------------------------</logmessage>
      <loglevel>Basic</loglevel>
      <logsubject>LOG</logsubject>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>144</yloc>
    </entry>
    <entry>
      <name>Get process information</name>
      <description />
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id />
      <filename>${Internal.Entry.Current.Directory}/m_get_uuid.ktr</filename>
      <transname />
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile />
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name />
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>368</yloc>
    </entry>
    <entry>
      <name>Process ID</name>
      <description />
      <type>WRITE_TO_LOG</type>
      <logmessage>#-----------------------------------------------------------
# UUID
#-----------------------------------------------------------
UUID == ${UUID}
#-----------------------------------------------------------</logmessage>
      <loglevel>Basic</loglevel>
      <logsubject>LOG</logsubject>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>240</xloc>
      <yloc>480</yloc>
    </entry>
    <entry>
      <name>Fail extracting data</name>
      <description />
      <type>ABORT</type>
      <message />
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>656</xloc>
      <yloc>592</yloc>
    </entry>
    <entry>
      <name>Delete temp</name>
      <description />
      <type>DELETE_FOLDERS</type>
      <arg_from_previous>N</arg_from_previous>
      <success_condition>success_if_no_errors</success_condition>
      <limit_folders>10</limit_folders>
      <fields>
        <field>
          <name>${GLOVE_TEMP}/${DATE}/named_query/${UUID}/</name>
        </field>
        <field>
          <name>${GLOVE_TEMP}/${DATE}/database/${UUID}/</name>
        </field>
        <field>
          <name>${GLOVE_TEMP}/${DATE}/file/${UUID}/</name>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>528</xloc>
      <yloc>592</yloc>
    </entry>
    <entry>
      <name>Success</name>
      <description />
      <type>SUCCESS</type>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>656</xloc>
      <yloc>480</yloc>
    </entry>
    <entry>
      <name>Delete trash</name>
      <description />
      <type>DELETE_FOLDERS</type>
      <arg_from_previous>N</arg_from_previous>
      <success_condition>success_if_no_errors</success_condition>
      <limit_folders>10</limit_folders>
      <fields>
        <field>
          <name>${GLOVE_TEMP}/${YESTERDAY}/</name>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>656</xloc>
      <yloc>368</yloc>
    </entry>
    <entry>
      <name>Cleanup window</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${HOUR}</variablename>
      <fieldtype>number</fieldtype>
      <mask />
      <comparevalue>${GLOVE_CLEANUP_HOUR}</comparevalue>
      <minvalue>19</minvalue>
      <maxvalue>20</maxvalue>
      <successcondition>equal</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>528</xloc>
      <yloc>368</yloc>
    </entry>
    <entry>
      <name>Check if a folder exists</name>
      <description />
      <type>FILE_EXISTS</type>
      <filename>${GLOVE_TEMP}/${YESTERDAY}/</filename>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>528</xloc>
      <yloc>480</yloc>
    </entry>
    <entry>
      <name>Get Parameters</name>
      <description />
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id />
      <filename>${Internal.Entry.Current.Directory}/m_get_parameters.ktr</filename>
      <transname />
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile />
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name />
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration />
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>80</xloc>
      <yloc>480</yloc>
    </entry>
    <entry>
      <name>Is Debug ?</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${DEBUG}</variablename>
      <fieldtype>number</fieldtype>
      <mask />
      <comparevalue>1</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>equal</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>592</yloc>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>START</from>
      <to>Setup</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Setup</from>
      <to>Metadata folder exists?</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Metadata folder exists?</from>
      <to>Get process information</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Process ID</from>
      <to>Module selector</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Delete temp</from>
      <to>Fail extracting data</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Cleanup window</from>
      <to>Delete trash</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Delete trash</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Cleanup window</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Module selector</from>
      <to>Check if a folder exists</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Check if a folder exists</from>
      <to>Cleanup window</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Check if a folder exists</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Get process information</from>
      <to>Get Parameters</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Get Parameters</from>
      <to>Process ID</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Module selector</from>
      <to>Is Debug ?</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Is Debug ?</from>
      <to>Delete temp</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
      <attribute>
        <key>Pentaho local</key>
        <value>{"children":[{"children":[],"id":"server","value":null},{"children":[],"id":"clustered","value":"N"},{"children":[],"id":"name","value":"Pentaho local"},{"children":[],"id":"description","value":null},{"children":[],"id":"pentaho","value":"N"},{"children":[],"id":"readOnly","value":"Y"},{"children":[],"id":"sendResources","value":"N"},{"children":[],"id":"logRemoteExecutionLocally","value":"N"},{"children":[],"id":"remote","value":"N"},{"children":[],"id":"local","value":"Y"},{"children":[],"id":"showTransformations","value":"N"}],"id":"Pentaho local","value":null,"name":"Pentaho local","owner":null,"ownerPermissionsList":[]}</value>
      </attribute>
    </group>
  </attributes>
</job>
